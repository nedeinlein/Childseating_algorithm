{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6167b863-3114-4ed8-aeb8-41338b1651f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Readme:\n",
    "\n",
    "Purpose: Automation of Child Seating \n",
    "- Note this code will not run as is and needs to have data and options inserted in varius places to make it run.\n",
    "\n",
    "Current Version:\n",
    "- 1/1/24 - code cleanup and optimization/ making generic for git\n",
    "- 7/17/23 - through flight restructure and integration\n",
    "- 6/20/23 - Initial Creation\n",
    "\n",
    "Section Summary:\n",
    " - Imports and connections\n",
    " - Define Functions\n",
    " - Universal Datasets\n",
    "    - Date Range Setup\n",
    " - Heracles Protocol\n",
    "    - Query to find unassigned Children\n",
    "    - Pull all PNR's Related to families\n",
    "    - Seat Purchase Query\n",
    "    - Identfy Children in Exit Rows to Reassign\n",
    " - Through Flights\n",
    "   - Identify all through flights\n",
    "   - Create through flight seat groups\n",
    "   - Create through flight passenger groups\n",
    "   - Through Seat Assignment\n",
    "      - Expand out through seat assignment to both legs\n",
    " - Single Leg Assignments\n",
    "    - Create single leg seat groups (removing already assigned)\n",
    "    - Create single leg pax groups\n",
    "    - Single leg pax assignment\n",
    " - Seat Upsale\n",
    "    - Pull in pax selected for upsale (remove all through assignments)\n",
    "    - Split flights into test and control groups\n",
    "    - Split those on test flights into test and control groups\n",
    " - Pax Number Assignment\n",
    "    - Passenger Number query\n",
    " - Final Assembly\n",
    "    - Query for passenger info and contact\n",
    "\n",
    " Inputs:\n",
    " - passenger import from sql query\n",
    " - airplane seatmap\n",
    "\n",
    " \n",
    " Outputs:\n",
    "  - automated list for import to api for assignment\n",
    "\n",
    " Definitions:\n",
    "  - PNR/Recordlocator - identifier for booking\n",
    "  - Pax is passenger\n",
    "  - Unit designator is a seat on an aircraft\n",
    "  - LF or Loadfactor is how full the plane is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0267fa57-1c8c-46bb-b3c9-addcff4ddbfa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd776b67-cdd4-4f93-b557-1bb08dc7485b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "from datetime import date, datetime, timedelta\n",
    "from pyspark.sql import functions as F, Window\n",
    "from pyspark.sql.types import IntegerType,BooleanType,DateType, StringType\n",
    "from pyspark.sql.functions import dayofweek, concat_ws, array, sort_array, month, year,date_format, to_date, current_date\n",
    "from pyspark.sql.functions import concat_ws, array, sort_array\n",
    "from pyspark.sql.functions import month\n",
    "\n",
    "'''insert database connector as well'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "472197a4-b0d8-4e47-a296-ba0b960dda9e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53f309c6-94d5-4b24-9795-dda17b2b5068",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''Define all universal functions that will be used in the algorithm'''\n",
    "\n",
    "''' function to change group items and create list column'''    \n",
    "def groupset(df, group, target, colname):\n",
    "    df = df.groupby(group).agg(F.concat_ws(\",\", F.collect_list(target)))\n",
    "    df = df.withColumnRenamed(df.columns[-1],colname)\n",
    "    df = df.withColumn(colname, F.split(colname, ','))\n",
    "    return df\n",
    "\n",
    "'''function for collecting dataset of passenger numbers'''\n",
    "def paxnumbers(df):\n",
    "    df = df[['PassengerID','RecordLocator']]\n",
    "    df = df.dropDuplicates(subset = ['PassengerID'])\n",
    "    df = df.sort(F.col('PassengerID'))\n",
    "    df = df.groupby('RecordLocator').agg(F.concat_ws(\",\", F.collect_list(df.PassengerID))) #aggregate passenger ids into single row per PNR\n",
    "    df = df.withColumnRenamed('concat_ws(,, collect_list(PassengerID))','PassengerID')\n",
    "    df = df.withColumn('PassengerID', F.split('PassengerID', ',')) #turn string to list\n",
    "    df = df.withColumn('PassengerNumber',F.lit('0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40')) #add passenger number list\n",
    "    df = df.withColumn('PaxNumMax',F.size(F.col('PassengerID'))).withColumn('PassengerNumber',F.split('PassengerNumber', ',')) #pull max number of passenger id's needed\n",
    "    df = df.withColumn(\"PassengerNumber\", F.expr(\"slice(PassengerNumber,1, PaxNumMax)\")) #pull correct number of passenger numbers needed for each PNR\n",
    "    people = df.select(df.RecordLocator,F.explode(df.PassengerID)) #explode list for pax ids\n",
    "    no = df.select(df.RecordLocator,F.explode(df.PassengerNumber)) #explode list for pax #\n",
    "    people = people.withColumn(\"id\", F.monotonically_increasing_id()).withColumnRenamed('col','PassengerID') #give unique id per row\n",
    "    no = no.withColumn(\"id\", F.monotonically_increasing_id()).withColumnRenamed('col','PassengerNumber') #give unique id per row\n",
    "    df = people.join(no,['RecordLocator','ID']) #join on unique id and record locator\n",
    "    df = df.drop('id')\n",
    "    df = df.withColumn('PassengerNumber', F.col('PassengerNumber').cast(IntegerType())).withColumn('PassengerID', F.col('PassengerID').cast(IntegerType()))\n",
    "    return df\n",
    "\n",
    "''' builds out list of sequential seats dataset'''\n",
    "def seqseats(df,seat_priority,type):\n",
    "    if type =='through':\n",
    "        groupby = ['InventoryKey','Row']\n",
    "        cols = ['InventoryKey','Row','EmptySeats','Seats','Priority','rowcap','seqcap','seqseats']\n",
    "    else:\n",
    "        groupby = ['InventoryLegID','Row']\n",
    "        cols = ['InventoryLegID','Row','EmptySeats','Seats','Priority','rowcap','seqcap','seqseats']\n",
    "    availableseats = df.join(seat_priority,['Equipment','LF','UnitDesignator']) #join with seat priority\n",
    "    availableseats = availableseats.withColumn('Row',F.expr(\"substring(UnitDesignator, 1, length(UnitDesignator)-1)\")) #pull row number\n",
    "    availableseats = availableseats.withColumn('Seats',F.col('UnitDesignator').substr(-1,1)) #pull seat letter\n",
    "    availableseats_pr = availableseats.groupby(groupby).sum('Priority')\n",
    "    availableseats_pr = availableseats_pr.withColumnRenamed('sum(Priority)','Priority')\n",
    "    availableseats = availableseats.sort(F.col('Priority'))\n",
    "    availableseats_seats = availableseats.groupby(groupby).agg(F.concat_ws(\",\", F.collect_list(availableseats.UnitDesignator)),F.concat_ws(\",\", F.collect_list(availableseats.Seats))) #put sum seats into row\n",
    "    availableseats = availableseats_seats.join(availableseats_pr,groupby)\n",
    "    availableseats = availableseats.withColumnRenamed('concat_ws(,, collect_list(UnitDesignator))','EmptySeats').withColumnRenamed('concat_ws(,, collect_list(Seats))','Seats')\n",
    "    availableseats = availableseats.withColumn('EmptySeats', F.split('EmptySeats', ',')) #string to list\n",
    "    availableseats = availableseats.withColumn('rowcap',F.size(F.col('EmptySeats'))) #create row capacity size\n",
    "    availableseats = availableseats.withColumn('Priority',F.col('Priority')/F.col('rowcap'))\n",
    "    availableseats = availableseats.withColumn('seqseats', \n",
    "                                            F.when(F.col('Seats').contains('A,B,C,D,E,F'),'A,B,C,D,E,F')\n",
    "                                            .when(F.col('Seats').contains('F,E,D,C,B,A'),'F,E,D,C,B,A')\n",
    "                                            .when(F.col('Seats').contains('B,C,D,E,F'),'B,C,D,E,F')\n",
    "                                            .when(F.col('Seats').contains('F,E,D,C,B'),'F,E,D,C,B')\n",
    "                                            .when(F.col('Seats').contains('A,B,C,D,E'),'A,B,C,D,E')\n",
    "                                            .when(F.col('Seats').contains('E,D,C,B,A'),'E,D,C,B,A')\n",
    "                                            .when(F.col('Seats').contains('C,D,E,F'),'C,D,E,F')\n",
    "                                            .when(F.col('Seats').contains('F,E,D,C'),'F,E,D,C')\n",
    "                                            .when(F.col('Seats').contains('A,B,C,D'),'A,B,C,D')\n",
    "                                            .when(F.col('Seats').contains('D,C,B,A'),'D,C,B,A')\n",
    "                                            .when(F.col('Seats').contains('B,C,D,E'),'B,C,D,E')\n",
    "                                            .when(F.col('Seats').contains('E,D,C,B'),'E,D,C,B')\n",
    "                                            .when(F.col('Seats').contains('A,B,C'),'A,B,C')\n",
    "                                            .when(F.col('Seats').contains('C,B,A'),'C,B,A')\n",
    "                                            .when(F.col('Seats').contains('B,C,D'),'B,C,D')\n",
    "                                            .when(F.col('Seats').contains('D,C,B'),'D,C,B')\n",
    "                                            .when(F.col('Seats').contains('C,D,E'),'C,D,E')\n",
    "                                            .when(F.col('Seats').contains('E,D,C'),'E,D,C')\n",
    "                                            .when(F.col('Seats').contains('D,E,F'),'D,E,F')\n",
    "                                            .when(F.col('Seats').contains('F,E,D'),'F,E,D')\n",
    "                                            .when(F.col('Seats').contains('E,F'),'E,F')\n",
    "                                            .when(F.col('Seats').contains('F,E'),'F,E')\n",
    "                                            .when(F.col('Seats').contains('D,E'),'D,E')\n",
    "                                            .when(F.col('Seats').contains('E,D'),'E,D')\n",
    "                                            .when(F.col('Seats').contains('A,B'),'A,B')\n",
    "                                            .when(F.col('Seats').contains('B,A'),'B,A')\n",
    "                                            .when(F.col('Seats').contains('B,C'),'B,C')\n",
    "                                            .when(F.col('Seats').contains('C,B'),'C,B')\n",
    "                                            .when(F.col('Seats').contains('A'),'A')\n",
    "                                            .when(F.col('Seats').contains('B'),'B')\n",
    "                                            .when(F.col('Seats').contains('C'),'C')\n",
    "                                            .when(F.col('Seats').contains('D'),'D')\n",
    "                                            .when(F.col('Seats').contains('E'),'E')\n",
    "                                            .when(F.col('Seats').contains('F'),'F').otherwise(F.lit('none'))) #create sequential list of seats\n",
    "    availableseats = availableseats.withColumn('seqseats', F.split('seqseats', ',')) #string to list\n",
    "    availableseats = availableseats.withColumn('seqcap',F.size(F.col('seqseats'))) #create row capacity size\n",
    "    availableseats = availableseats.withColumn(\"seqseats\",F.expr('''concat_ws(',',transform(seqseats, x->concat(Row,x)))''')) #combine row with sequential list\n",
    "    availableseats = availableseats.withColumn('seqseats', F.split('seqseats', ',')) #string to list\n",
    "    availableseats = availableseats[cols]\n",
    "    availableseats = availableseats.filter(F.col('seqcap')>0)\n",
    "    return availableseats\n",
    "\n",
    "'''builds out passenger groups'''\n",
    "def paxgroup(familyPNR,type):\n",
    "    if type =='through':\n",
    "        groupby = ['InventoryKey','RecordLocator']\n",
    "        cols1 = ['InventoryKey','RecordLocator','SquadLeader','Hobbits']\n",
    "        cols2 = ['InventoryKey','RecordLocator','AdultosGroup1','KidGroup1']\n",
    "        cols3 = ['InventoryKey','RecordLocator','AdultosGroup2','KidGroup2']\n",
    "    else:\n",
    "        groupby = ['InventoryLegID','RecordLocator']\n",
    "        cols1 = ['InventoryLegID','RecordLocator','SquadLeader','Hobbits']\n",
    "        cols2 = ['InventoryLegID','RecordLocator','AdultosGroup1','KidGroup1']\n",
    "        cols3 = ['InventoryLegID','RecordLocator','AdultosGroup2','KidGroup2']\n",
    "\n",
    "    #removal of single passengers\n",
    "    multi_pax_filter = familyPNR.groupby(groupby).count() #groupby pnr to get pax count\n",
    "    multi_pax_filter = multi_pax_filter.filter(F.col('count') > 1) #select only pnrs with more than one pax\n",
    "    multi_pax_filter = multi_pax_filter.drop('count')\n",
    "    familyPNR = familyPNR.join(multi_pax_filter,groupby)\n",
    "\n",
    "    #removal of kid only PNRs\n",
    "    kids_only = familyPNR.groupby(groupby).max('Age') #groupby pnr to get max age\n",
    "    kids_only = kids_only.filter(F.col('max(Age)')>17)\n",
    "    kids_only = kids_only.drop('max(Age)')\n",
    "    familyPNR = familyPNR.join(kids_only,groupby)\n",
    "\n",
    "    #remove assigned/purchased\n",
    "    familyPNR = familyPNR.filter(F.col('UnitDesignator')=='')\n",
    "    #create PaxType \n",
    "    familyPNR = familyPNR.withColumn('KidCount', F.when(F.col('Age')<14,1).otherwise(0))\n",
    "    familyPNR = familyPNR.withColumn('Adults', F.when(F.col('Age')>=18,1).otherwise(0))\n",
    "    familyPNR = familyPNR.withColumn('Adultos', F.when(F.col('Adults')==1,F.col('PassengerID')).otherwise(F.lit('x')))\n",
    "    familyPNR = familyPNR.withColumn('Hobbits', F.when(F.col('KidCount')==1,F.col('PassengerID')).otherwise(F.lit('x')))\n",
    "\n",
    "    #pull all adults\n",
    "    adultos = familyPNR.filter(F.col('Adults')>0)\n",
    "    adultos = adultos.sort(F.col('Age').desc()) #sort list so oldest adult is most likely to be paired with a kid\n",
    "    adultos = adultos.groupby(groupby).agg(F.concat_ws(\",\", F.collect_list(adultos.Adultos))) #group all adults into single row per pnr\n",
    "    adultos = adultos.withColumnRenamed('concat_ws(,, collect_list(Adultos))','Adultos')\n",
    "    adultos = adultos.withColumn('Adultos', F.regexp_replace('Adultos',\"x,\",\"\")).withColumn('Adultos', F.regexp_replace('Adultos',\",x\",\"\")).withColumn('Adultos', F.regexp_replace('Adultos',\"x\",\"\")) #replace null values in string\n",
    "    adultos = adultos.withColumn('Adultos', F.split('Adultos', ',')) #convert string to list\n",
    "\n",
    "    #pull all kids\n",
    "    hobbitses = familyPNR.filter(F.col('Hobbits')>0)\n",
    "    hobbitses = hobbitses.sort(F.col('Age')) #sort so youngest kids come first\n",
    "    hobbitses = hobbitses.groupby(groupby).agg(F.concat_ws(\",\", F.collect_list(hobbitses.Hobbits))) #group all adults into single row per pnr\n",
    "    hobbitses = hobbitses.withColumnRenamed('concat_ws(,, collect_list(Hobbits))','Hobbits')\n",
    "    hobbitses = hobbitses.withColumn('Hobbits', F.regexp_replace('Hobbits',\"x,\",\"\")).withColumn('Hobbits', F.regexp_replace('Hobbits',\",x\",\"\")).withColumn('Hobbits', F.regexp_replace('Hobbits',\"x\",\"\")).withColumn('Hobbits', F.regexp_replace('Hobbits',\"\",'')) #replace null values in string\n",
    "    hobbitses = hobbitses.withColumn('Hobbits', F.split('Hobbits', ',')) #convert string to list\n",
    "\n",
    "    #recombine data\n",
    "    familyPNR = familyPNR.groupby(groupby).sum('KidCount','Adults')\n",
    "    familyPNR = familyPNR.withColumnRenamed('sum(KidCount)','KidCount').withColumnRenamed('sum(Adults)','GroupMax')\n",
    "    thegrid = familyPNR.join(adultos, groupby) #add in adult lists\n",
    "    thegrid = thegrid.join(hobbitses, groupby) #add in kid lists\n",
    "\n",
    "    #single parent flying with multiple kids\n",
    "    singleparent = thegrid.filter(F.col('GroupMax')==1) #number of possible groups is 1\n",
    "    singleparent = singleparent.withColumnRenamed('Adultos','SquadLeader')\n",
    "    singleparent = singleparent.withColumn(\"SquadLeader\",concat_ws(\",\",F.col(\"SquadLeader\")))\n",
    "    singleparent = singleparent[cols1]\n",
    "\n",
    "    #only one child, just add adult\n",
    "    pairedriders = thegrid.filter(F.col('KidCount')==1) #filter to pnr with only one kid\n",
    "    pairedriders = pairedriders.filter(F.col('GroupMax')>1)  #filter to pnr's taken care of by sad parent group\n",
    "    pairedriders = pairedriders.withColumn('Adultos',F.array([F.col(\"Adultos\")[0]]))\n",
    "    pairedriders = pairedriders.withColumnRenamed('Adultos','SquadLeader')\n",
    "    pairedriders = pairedriders.withColumn(\"SquadLeader\",concat_ws(\",\",F.col(\"SquadLeader\")))\n",
    "    pairedriders = pairedriders[cols1]\n",
    "\n",
    "    #PNR with more than one group in them\n",
    "    groups = thegrid.filter(F.col('GroupMax')>1) #groups with more than one adult\n",
    "    groups = groups.filter(F.col('KidCount')>1) #groups with more than one kid\n",
    "    groups = groups.filter(F.col('KidCount')<=10) #groups max at 5 kids in each group. Any larger and we should not be assigning as that would be larger than a row.\n",
    "    groups = groups.withColumn('AdultosGroup1',F.col(\"Adultos\")[0]).withColumn('AdultosGroup2',F.col(\"Adultos\")[1])\n",
    "    groups = groups.withColumn(\"KidGroup1\", F.when(F.col('KidCount')==3,F.array([F.col(\"Hobbits\")[0],F.col(\"Hobbits\")[2]]))\n",
    "                            .when(F.col('KidCount')==4,F.array([F.col(\"Hobbits\")[0],F.col(\"Hobbits\")[2]]))\n",
    "                            .when(F.col('KidCount')==5,F.array([F.col(\"Hobbits\")[0],F.col(\"Hobbits\")[2],F.col(\"Hobbits\")[4]]))\n",
    "                            .when(F.col('KidCount')==6,F.array([F.col(\"Hobbits\")[0],F.col(\"Hobbits\")[2],F.col(\"Hobbits\")[4]]))\n",
    "                            .when(F.col('KidCount')==7,F.array([F.col(\"Hobbits\")[0],F.col(\"Hobbits\")[2],F.col(\"Hobbits\")[4],F.col(\"Hobbits\")[6]]))\n",
    "                            .when(F.col('KidCount')==8,F.array([F.col(\"Hobbits\")[0],F.col(\"Hobbits\")[2],F.col(\"Hobbits\")[4],F.col(\"Hobbits\")[6]]))\n",
    "                            .when(F.col('KidCount')==9,F.array([F.col(\"Hobbits\")[0],F.col(\"Hobbits\")[2],F.col(\"Hobbits\")[4],F.col(\"Hobbits\")[6],F.col(\"Hobbits\")[8]]))\n",
    "                            .when(F.col('KidCount')==10,F.array([F.col(\"Hobbits\")[0],F.col(\"Hobbits\")[2],F.col(\"Hobbits\")[4],F.col(\"Hobbits\")[6],F.col(\"Hobbits\")[8]]))\n",
    "                            .otherwise(F.array([F.col(\"Hobbits\")[0]]))) ## divide groups up by size into two groups of kids\n",
    "    groups = groups.withColumn(\"KidGroup2\", F.when(F.col('KidCount')==3,F.array([F.col(\"Hobbits\")[1]]))\n",
    "                            .when(F.col('KidCount')==4,F.array([F.col(\"Hobbits\")[1],F.col(\"Hobbits\")[3]]))\n",
    "                            .when(F.col('KidCount')==5,F.array([F.col(\"Hobbits\")[1],F.col(\"Hobbits\")[3]]))\n",
    "                            .when(F.col('KidCount')==6,F.array([F.col(\"Hobbits\")[1],F.col(\"Hobbits\")[3],F.col(\"Hobbits\")[5]]))\n",
    "                            .when(F.col('KidCount')==7,F.array([F.col(\"Hobbits\")[1],F.col(\"Hobbits\")[3],F.col(\"Hobbits\")[5]]))\n",
    "                            .when(F.col('KidCount')==8,F.array([F.col(\"Hobbits\")[1],F.col(\"Hobbits\")[3],F.col(\"Hobbits\")[5],F.col(\"Hobbits\")[7]]))\n",
    "                            .when(F.col('KidCount')==9,F.array([F.col(\"Hobbits\")[1],F.col(\"Hobbits\")[3],F.col(\"Hobbits\")[5],F.col(\"Hobbits\")[7]]))\n",
    "                            .when(F.col('KidCount')==10,F.array([F.col(\"Hobbits\")[1],F.col(\"Hobbits\")[3],F.col(\"Hobbits\")[5],F.col(\"Hobbits\")[7],F.col(\"Hobbits\")[9]]))\n",
    "                            .otherwise(F.array([F.col(\"Hobbits\")[1]]))) ## divide groups up by size into two groups of kids\n",
    "    groups = groups.withColumn('Hobbits', F.array([F.col('KidGroup1'),F.col('KidGroup2')]))\n",
    "    groups1 = groups[cols2] #split group 1 from group 2\n",
    "    groups1 = groups1.withColumnRenamed('AdultosGroup1','Squadleader').withColumnRenamed('KidGroup1','Hobbits')\n",
    "    groups2 = groups[cols3] #split group 2 from group 1\n",
    "    groups2 = groups2.withColumnRenamed('AdultosGroup2','Squadleader').withColumnRenamed('KidGroup2','Hobbits')\n",
    "    df3 = groups1.union(groups2) #recombine both parts of the group dataset\n",
    "    df3 = df3.union(singleparent) #combine other data sets\n",
    "    df3 = df3.union(pairedriders) #combine other data sets\n",
    "    df3 = df3.withColumn(\"Hobbits\",concat_ws(\",\",F.col(\"Hobbits\")))\n",
    "    df3 = df3.withColumn('the_squad', F.concat(F.col('Squadleader'),F.lit(\",\"),F.col('Hobbits'))) #combine kid and adult columns to form a squad that will be assigned\n",
    "    df3 = df3.withColumn('the_squad', F.split('the_squad', ','))\n",
    "    df3 = df3.withColumn('squadsize',F.size(F.col('the_squad'))) #pull the size of the squad to be tetrised together\n",
    "    df3 = df3.drop('Squadleader','Hobbits') #remove unneccessary columns\n",
    "    df3 = df3.filter(F.col('squadsize')<7)\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e156ad8-eafe-4afd-87c0-cd46b87cb8ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Universal Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48b1fe97-3811-46a4-bbbc-12dc50e4ea84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-14 - 2023-12-26\n"
     ]
    }
   ],
   "source": [
    "#set Date parameter\n",
    "Date = date.today()\n",
    "Datestr = Date.strftime('%Y-%m-%d')\n",
    "MinDate = Date+timedelta(days = 2) \n",
    "MinDate = MinDate.strftime('%Y-%m-%d')\n",
    "FutureDate = Date+timedelta(days = 14)\n",
    "FutureDate = FutureDate.strftime('%Y-%m-%d')\n",
    "#the max date we pull from is 2 days from now so that we are not assigning seats \n",
    "#into flights that are having people completing check-in\n",
    "print(MinDate, \"-\" ,FutureDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea2cbba9-43ee-4f82-8700-9c5e723c90b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Family Seating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ba48a80-4e12-4fe0-b3a1-f788e046230a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Query For Unassigned Children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f11b5e82-13f8-475d-9827-06950b1d510b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' This query looks for all passengers under the age of 13. \n",
    "It will then be put into a dataframe that removes duplications on RecordLocator. \n",
    "This can then be used with a simple inner join in order to filter all passengers down to \n",
    "just passengers with a child on the PNR.'''\n",
    "\n",
    "#to be assigned filter\n",
    "kid_check_PNR_query = \"\"\"Insert Query for navitaire that pulls all passengers 13 and younger\"\"\"\n",
    "kid_check = spark.read.jdbc(url=jdbcUrl, table=kid_check_PNR_query)\n",
    "#write to delta table for efficiency and getting off sql server\n",
    "kid_check.write.mode(\"overwrite\").saveAsTable(\"child_seating_filter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c452e520-ac5b-4dbe-bf62-b577f7ca67f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Pull All PNR's Related to Families "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff81db87-8214-4860-94a5-e78f4c015e69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''Full passenger query that pulls the passengers as well as their current seat assignment'''\n",
    "\n",
    "#query for PNR's with children\n",
    "familyPNR_query = \"\"\" Query for all passengers that are flying during that time period\"\"\"\n",
    "familyPNR = spark.read.jdbc(url=jdbcUrl, table=familyPNR_query)\n",
    "#write to delta table for efficiency and getting off sql server\n",
    "familyPNR.write.mode(\"overwrite\").saveAsTable(\"all_passengers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1980e1c0-007a-46a5-87fd-e75f7ddf673d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''Create a filter for PNR's with children on them as described above'''\n",
    "\n",
    "filt = spark.sql('select * from child_seating_filter') #pull in child pax query\n",
    "filt = filt[['RecordLocator']] # reduce data set to just recordlocator\n",
    "# remove duplication in order to ensure that we are not creating duplicate records in final dataset\n",
    "filt = filt.dropDuplicates(subset = ['RecordLocator']) \n",
    "familyPNR = spark.sql('select * from all_passengers') # pull in full pax query\n",
    "# join with family filter to remove all PNR that do not have a child on them\n",
    "familyPNR = familyPNR.join(filt, ['RecordLocator']) \n",
    "familyPNR.write.mode(\"overwrite\").saveAsTable(\"all_passengers\") # save new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15130664-903f-4b0b-9409-ed40a96d03c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Seat Purchases Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a142eb6e-9f8a-4e69-a0a8-0715b36de2af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' Passengers who picked a seat will already have an assignment. \n",
    "However, this is also true for all passengers that we have assigned. \n",
    "To differentiate and make sure we are not moving any passenger that has purchased a seat, \n",
    "we will use this dataset to get a list of pax that purchased \n",
    "and remove them from our assign dataset so we do not have to worry about reimbursement for moving one of these pax.'''\n",
    "\n",
    "#core ancillary query\n",
    "core_query = \"\"\"sql query to pull purchased seats\"\"\"\n",
    "\n",
    "core = spark.read.jdbc(url=jdbcUrl, table=core_query)\n",
    "core = core.write.mode(\"overwrite\").saveAsTable(\"purchased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c908f6d9-e61c-4b69-b7fa-8d36010f1854",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''This section takes the the seat query and cleans it. \n",
    "The final step will be merging the final data set from this with the to assign dataset using a left_anti join\n",
    "to remove pax from the assign dataset that have purchased seats'''\n",
    "\n",
    "core = spark.sql('select * from purchased')\n",
    "core = core.fillna(0, subset = 'InventoryLegID') #fill in any missing inventorylegid's with 0 to denote not a flight\n",
    "# pull all discounts and surcharges\n",
    "d1 = core.filter((F.col('ChargeDetail').contains(\"\"\"(D)\"\"\")) | (F.col('ChargeDetail').contains(\"Fee OR\")))\n",
    "# pull all core fees\n",
    "core2 = core.filter((~F.col('ChargeDetail').contains(\"\"\"(D)\"\"\")) & (~F.col('ChargeDetail').contains(\"Fee OR\")))\n",
    "# multiply discounts and surcharges by -1 to correct sign\n",
    "d1 = d1.withColumn('ChargeAmount', F.col('chargeAmount')*-1)\n",
    "purchased = purchased.union(d1) #recombine datasets\n",
    "purchased = purchased.groupby('PassengerID','ChargeCode','FeeNumber','RecordLocator','BookingID','DepartureDate','SegmentNumber','DepartureStation','ArrivalStation','InventoryLegID').sum('ChargeAmount') #sum charges based on grouping to get actual charge amount\n",
    "#reduce down to necessary columns to filter out the assign dataset\n",
    "purchased = purchased[['PassengerID','RecordLocator','InventoryLegID']]\n",
    "purchased = purchased.dropDuplicates() #drop duplication\n",
    "familyPNR = spark.sql('select * from all_passengers')\n",
    "#left anti to remove passenegers who purchased\n",
    "familyPNR = familyPNR.join(purchased, ['PassengerID','RecordLocator','InventoryLegID'], \"left_anti\") \n",
    "familyPNR.write.mode(\"overwrite\").saveAsTable(\"all_passengers\") #write to delta updated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc1bc07d-56ae-4c8d-b0fa-75e3c7976410",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Children in Exit Rows Reassign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eafe0f8-d09a-4552-9669-9116f61939d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' Before assignment we also want to add in and identify all passengers that didn't purchase a seat that\n",
    "are in the exit rows. This is most often the result of an equipment change. To fix this we will first identify them,\n",
    "then remove their seat assignment on the assign dataset.'''\n",
    "\n",
    "exit_row = spark.sql('select * from all_passengers')\n",
    "exit_row_cols = exit_row.columns # pull exit row columns to put data back in correct order later\n",
    "exit_row = exit_row.withColumn('Row',F.expr(\"substring(UnitDesignator, 1, length(UnitDesignator)-1)\")) #pull row number\n",
    "#this creates and identifier combining the plane type and the row. This will then be filtered\n",
    "exit_row = exit_row.withColumn('ExitFilter', F.concat_ws(\"-\",exit_row.Capacity,exit_row.Row))\n",
    "# pull all families with seats in exit rows\n",
    "exit_row = exit_row.filter(F.col('ExitFilter').isin('''<insert list of exit row seats and equipment types>'''))\n",
    "\n",
    "'''Finally filter out exit row PNR's. We only want to reassign if the group in the exit row has a kid in it.'''\n",
    "kids = exit_row.filter(F.col('Age')<15)\n",
    "kids = kids[['RecordLocator','Row']]\n",
    "kids = kids.dropDuplicates()\n",
    "exit_row = exit_row.join(kids, ['RecordLocator','Row'])# join on kids filter\n",
    "exit_row = exit_row.sort(F.col('RecordLocator'),F.col('Age').desc())\n",
    "# remove the existing seat assignment from the pax data so it can be reassigned as if it had no assignment\n",
    "exit_row = exit_row.withColumn('UnitDesignator', F.lit('')) \n",
    "exit_row = exit_row[[exit_row_cols]] # re-match column order for main dataset\n",
    "#create filter of what rows from original dataset to removed so there is no duplication \n",
    "exit_row_peeps = exit_row[['PassengerID','InventoryLegID']] \n",
    "familyPNR = spark.sql('select * from all_passengers')\n",
    "familyPNR = familyPNR.join(exit_row_peeps, ['PassengerID','InventoryLegID'], 'left_anti')\n",
    "familyPNR = familyPNR[[exit_row_cols]]\n",
    "familyPNR = familyPNR.union(exit_row) # add in new exit row that we will reassign\n",
    "familyPNR.write.mode(\"overwrite\").saveAsTable(\"all_passengers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94d65b48-54e7-4f4d-915e-f2979cb4623e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Seat Assignment Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b037d1b-69b0-4945-8f45-c764e9b6e57e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''this section imports the seat maps we have created and builds out the dataset of all possible seats\n",
    "that could be assigned for a given aircraft type and sorts them by the priority we wish to seat people in by load factor.'''\n",
    "\n",
    "# import seat maps\n",
    "seat_map = spark.sql('select * from <insert seat_map dataset>') #pull up seat map for each plane type\n",
    "# takes seat map and breaks into individual seats for each flight type\n",
    "seat_map = seat_map.withColumn('UnitDesignator', F.split('UnitDesignator', ',')).withColumn('Priority', F.split('Priority', ','))\n",
    "# build out list of all available seats for each aircraft type\n",
    "seat_priority_seats = seat_map.select(seat_map.Equipment,seat_map.LF,F.explode(seat_map.UnitDesignator).alias('UnitDesignator'))\n",
    "seat_priority_seats = seat_priority_seats.withColumn('id',F.monotonically_increasing_id())\n",
    "# build out list of seat priorities\n",
    "seat_priority_prio = seat_map.select(seat_map.Equipment,seat_map.LF,F.explode(seat_map.Priority).alias('Priority'))\n",
    "seat_priority_prio = seat_priority_prio.withColumn('id',F.monotonically_increasing_id())\n",
    "#merge data sets back with correct priority for seat and the seat designator\n",
    "seat_priority = seat_priority_seats.join(seat_priority_prio,['id','Equipment','LF'])\n",
    "seat_priority = seat_priority.drop('id')\n",
    "seat_priority = seat_priority.withColumn('Priority',F.col('Priority').cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81a8d840-5596-489c-a1d1-7c76f622bfc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "'''this section pulls all passengers and gives us a list of those with seats to subtract from our available seats\n",
    "as well as giving us a list of our current load factor for those flights'''\n",
    "\n",
    "takenseat_query = \"\"\" sql query to pull all seats that are currently taken for any reason\"\"\"\n",
    "\n",
    "takenseat = spark.read.jdbc(url=jdbcUrl, table=takenseat_query)\n",
    "takenseat.write.mode(\"overwrite\").saveAsTable(\"empty_seats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfecb679-d6b4-4350-bce8-93c7e8c96f02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Through Flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4019989e-60a6-4d66-b07c-a829482aa13b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''Begin subtracting seats taken from available. This gives us a list of usable seats we can assign. \n",
    "Seat Map is set as Low LF to ensure that we are not back loading the lower of the two LF plane legs'''\n",
    "\n",
    "'''Through flights or flights that have the same flight number and will have passengers on more than one segment of them\n",
    "need to be treated differently since we don't want to be moving passengers around if they are staying on same plane.\n",
    "Goal is to create a ven diagram of seats taken on both parts of the flight and leave only seats open on both parts to seat\n",
    "passengers that are on both legs. If a passenger is only on one leg it gets treated like a normal assignment'''\n",
    "\n",
    "#quick clean and aggregation of the passenger info to calculate load factor\n",
    "through = spark.sql('select * from empty_seats') #pull all passengers to be assigned\n",
    "through = through[['InventoryLegID','InventoryKey']] #pull inventory key and inventory leg combo\n",
    "# drop duplicates so that we can get the number of distinct inventorylegID's per InventoryKey\n",
    "through = through.dropDuplicates()\n",
    "#Create df of InventoryKeys with their count of InventoryLegs and a list column of their inventory keys.\n",
    "through = through.groupby('InventoryKey').agg(F.count('InventoryLegID').alias('count'),F.collect_list(through.InventoryLegID).alias('legs'))\n",
    "# according to networking we should never have a through flight with more than 2 legs.\n",
    "do_not_assign = through.filter(F.col('count')>2) \n",
    "through = through.filter(F.col('count')==2) #pull all through flights ours only have 2 legs.\n",
    "through_filt = through[['InventoryKey']]\n",
    "through_filt = through_filt.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c46f09-f90d-4260-bc1c-057d4767673d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Through Flight Seat Group Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "105076f0-368e-4431-9b6b-cb9862a01b2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''Using the through filter we will grab the through flights then filter out al seats taken on both of the inventory legs. This creates basically a ven diagram of the seats available between the two flights. We will assign only to these seats.'''\n",
    "\n",
    "#dataset pullin\n",
    "capacity = spark.sql('select * from empty_seats') \n",
    "#pull in list of all pax that have a seat and all that do not have a seat\n",
    "\n",
    "t_flight_info = capacity[['InventoryKey','Capacity']] #grab unique flightlist\n",
    "t_flight_info = t_flight_info.dropDuplicates()\n",
    "#hit flight list with the through flight filter\n",
    "t_flight_info = t_flight_info.join(through_filt, ['InventoryKey']) \n",
    "t_flight_info = t_flight_info.withColumnRenamed('Capacity','Equipment')\n",
    "#setting all flights as low loadfactor because pattern seems to be one leg is high the other is low.\n",
    "t_flight_info = t_flight_info.withColumn('LF', F.lit('Low')) \n",
    "t_flight_info = t_flight_info.join(seat_map, ['Equipment','LF']) #join seatmap to flights\n",
    "t_allseats = t_flight_info.select(t_flight_info.InventoryKey, t_flight_info.Equipment,t_flight_info.LF,F.explode(t_flight_info.UnitDesignator).alias('UnitDesignator')) #create unique row for each seat\n",
    "\n",
    "#split out seats that have already been chosen\n",
    "taken_seats = capacity.filter(F.col('UnitDesignator') != '') #filter out passengers without seats\n",
    "taken_seats = taken_seats[['InventoryKey','UnitDesignator']] #create dataset of flight and taken seats\n",
    "taken_seats = groupset(taken_seats,taken_seats.InventoryKey, taken_seats.UnitDesignator, 'Taken')\n",
    "#explode list into individual rows\n",
    "taken = taken_seats.select(taken_seats.InventoryKey,F.explode(taken_seats.Taken).alias('UnitDesignator')) \n",
    "\n",
    "\n",
    "#filter down to seats left\n",
    "#remove taken seats from seat map on each flight\n",
    "availableseats = t_allseats.join(taken, ['InventoryKey','UnitDesignator'], \"left_anti\") \n",
    "availableseats = seqseats(availableseats,seat_priority,'through') #use defined function to save code space\n",
    "\n",
    "availableseats.write.mode(\"overwrite\").saveAsTable(\"seat_flights_through\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db0e960e-140c-4af5-9ce5-4987e98e9a46",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Through Flight Pax Group Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26c65804-47c1-4faf-a0cd-8581dcdbe2e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''This takes the through filter and will grab out passengers that are only on both legs of the through flight. \n",
    "If they are not on both, they can be assigned like any other passenger.'''\n",
    "\n",
    "familyPNR = spark.sql('select * from all_passengers')\n",
    "through_family = familyPNR.join(through_filt, ['InventoryKey']) # hit passengers with through flight list\n",
    "# create count of how many legs of the through flight they are on\n",
    "through_family = through_family.groupby('PassengerID','InventoryKey').count()\n",
    "#get rid of passengers who are only on one leg of flight\n",
    "through_family = through_family.filter(F.col('count')==2).drop('count') \n",
    "#create final dataset for through pax based on passenger has to be on both legs and has to be a through flight\n",
    "familyPNR = familyPNR.join(through_family, ['PassengerID','InventoryKey']) \n",
    "familyPNR = familyPNR[['PassengerID','RecordLocator','InventoryKey','Age','UnitDesignator']]\n",
    "familyPNR = familyPNR.dropDuplicates()\n",
    "\n",
    "df = paxgroup(familyPNR,'through') # run defined function to build seat groups\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"seatgroups_through\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8836d7e3-c35b-4ab3-bbc7-683a8c19c51c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Through Seat Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "229e4b6c-3d95-4559-a52d-1f5c0f5af898",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Large Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de4c7ae7-5922-44b5-9948-4baf5b374870",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3 = spark.sql('select * from seatgroups_through') #pull seat groups\n",
    "df3 = df3.filter(F.col('squadsize')>3) #grab only groups large enough to go across the aisle\n",
    "#create column noting the minimum group size we will be assigning\n",
    "paxlist = df3.groupby('InventoryKey').agg(F.min('squadsize').alias('mincap')) \n",
    "flights = spark.sql('select * from seat_flights_through') #pull seat list\n",
    "flights = flights.join(paxlist, ['InventoryKey']) #filter flights to\n",
    "#remove temporarily any seat groups that are too small for the family groups being assigned\n",
    "flights = flights.filter(F.col('seqcap')>=F.col('mincap')) \n",
    "df3_window = Window.partitionBy(\"InventoryKey\") #create partition\n",
    "df3 = df3.sort(F.col('squadsize').desc())\n",
    "#create index of passenger groups resetting for each inventorykey\n",
    "df3_indexed = df3.withColumn(\"count\", F.count(\"*\").over(df3_window)).withColumn(\"id\", F.row_number().over(df3_window.orderBy(df3.squadsize.desc()))) \n",
    "flights_window = Window.partitionBy(\"InventoryKey\")\n",
    "flights = flights.sort(F.col('seqcap').desc())\n",
    " #create index of flights resetting for each inventorykey\n",
    "flights_indexed = flights.withColumn(\"count\", F.count(\"*\").over(flights_window)).withColumn(\"id\", F.row_number().over(flights_window.orderBy(flights.seqcap.desc())))\n",
    "assigned = flights_indexed.join(df3_indexed, ['id','InventoryKey']) #create assignments through joins\n",
    "assigned = assigned.drop('count')\n",
    "assigned = assigned.filter(F.col('seqcap')>=F.col('squadsize'))\n",
    "assigned.write.mode(\"overwrite\").saveAsTable(\"seat_assign_through\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ecdd66-04a8-4789-a202-f6c34f1187cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Data Clean for Main Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c98ce159-989a-43f7-b984-89a91993661d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "testassign = spark.sql('select * from seat_assign_through')\n",
    "testassign = testassign.withColumn('remainder', F.col('seqcap')-F.col('squadsize')) #calculate remainder number of seats\n",
    "#if seat remainder is > 0 and squad size is >=2 lock in assignment\n",
    "lockin = testassign.filter((F.col('remainder')>=0) & (F.col('squadsize')>=2)) \n",
    "lockin = lockin.withColumn(\"Seats\", F.when(F.col('squadsize')==2,F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1]]))\n",
    "                           .when(F.col('squadsize')==3,F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1],F.col(\"seqseats\")[2]]))\n",
    "                           .when(F.col('squadsize')==4,F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1],F.col(\"seqseats\")[2],F.col(\"seqseats\")[3]]))\n",
    "                           .when(F.col('squadsize')==5,F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1],F.col(\"seqseats\")[2],F.col(\"seqseats\")[3],F.col(\"seqseats\")[4]]))\n",
    "                           .otherwise(F.col('seqseats')))\n",
    "lockin.write.mode(\"overwrite\").saveAsTable(\"seat_lockin_through\")\n",
    "newseatlist = lockin.withColumn('unused', F.array_except('seqseats', 'seats'))\n",
    "newseatlist = newseatlist.select(newseatlist.InventoryKey,newseatlist.Row,newseatlist.Priority,newseatlist.rowcap,F.explode(newseatlist.unused)) #explode list into individual rows\n",
    "newseatlist = newseatlist.withColumn('Seats',F.col('col').substr(-1,1)) #pull seat letter\n",
    "newseatlist = newseatlist.groupby('InventoryKey','Row','Priority','rowcap').agg(F.concat_ws(\",\", F.collect_list(newseatlist.col)),F.concat_ws(\",\", F.collect_list(newseatlist.Seats))) #put sum seats into row\n",
    "newseatlist = newseatlist.withColumnRenamed('concat_ws(,, collect_list(col))','EmptySeats').withColumnRenamed('concat_ws(,, collect_list(Seats))','Seats')\n",
    "newseatlist = newseatlist.withColumn('seqseats', F.col('EmptySeats'))\n",
    "newseatlist = newseatlist.withColumn('EmptySeats', F.split('EmptySeats', ',')).withColumn('seqseats', F.split('seqseats', ',')) #string to list\n",
    "newseatlist = newseatlist.withColumn('seqcap',F.size(F.col('seqseats'))) #create row capacity size\n",
    "newseatlist = newseatlist[['InventoryKey','Row','EmptySeats','Seats','Priority','rowcap','seqcap','seqseats']]\n",
    "filt = newseatlist[['InventoryKey','Row']]\n",
    "old_list = spark.sql('select * from seat_flights_through') #pull seat list\n",
    "old_list = old_list.join(filt,['InventoryKey','Row'], how = 'left_anti')\n",
    "availableseats = newseatlist.union(old_list)\n",
    "availableseats = availableseats.filter(F.col('seqcap')>0)\n",
    "availableseats.write.mode(\"overwrite\").saveAsTable(\"seat_flights_through\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f4ce11c-a98c-47cc-81e9-c812097b282e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Main Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc817e94-0664-4951-a4c8-92b1eb335082",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' this section of code take the remaining open seats from the large group assignments and breaks them \n",
    "along the center aisle. This is to prevent any small groups of 2 or 3 from being separated by the aisle \n",
    "even if the seats are technically sequential. It then assigns passengers to this new seat grouping'''\n",
    "\n",
    "df3 = spark.sql('select * from seatgroups_through') #pull seat groups\n",
    "df3 = df3.filter(F.col('squadsize')<4)\n",
    "paxlist = df3.groupby('InventoryKey').agg(F.min('squadsize').alias('mincap'))\n",
    "flights = spark.sql('select * from seat_flights_through') #pull seat list\n",
    "flights = flights.withColumn(\"seqseats\", \\\n",
    "    F.when(F.col('Seats').contains('A,B,C'),F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1],F.col(\"seqseats\")[2]]))\n",
    "    .when(F.col('Seats').contains('F,E,D'),F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1],F.col(\"seqseats\")[2]]))\n",
    "    .when(F.col('Seats').contains('C,B,A'),F.array([F.element_at(F.col('seqseats'), -3), F.element_at(F.col('seqseats'), -2),F.element_at(F.col('seqseats'), -1)]))\n",
    "    .when(F.col('Seats').contains('D,E,F'),F.array([F.element_at(F.col('seqseats'), -3), F.element_at(F.col('seqseats'), -2),F.element_at(F.col('seqseats'), -1)]))\n",
    "    .when(F.col('Seats').contains('B,C'),F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1]]))\n",
    "    .when(F.col('Seats').contains('E,D'),F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1]]))\n",
    "    .when(F.col('Seats').contains('C,B'),F.array([ F.element_at(F.col('seqseats'), -2),F.element_at(F.col('seqseats'), -1)]))\n",
    "    .when(F.col('Seats').contains('D,E'),F.array([ F.element_at(F.col('seqseats'), -2),F.element_at(F.col('seqseats'), -1)]))\n",
    "    .when(F.col('Seats').contains('D,C'),F.array([F.col(\"seqseats\")[0]]))\n",
    "    .when(F.col('Seats').contains('C,D'),F.array([F.col(\"seqseats\")[0]]))\n",
    "    .otherwise(F.col(\"seqseats\")))\n",
    "flights = flights.withColumn('seqcap',F.size(F.col('seqseats'))) #create row capacity size\n",
    "flights = flights.join(paxlist, ['InventoryKey'])\n",
    "flights = flights.filter(F.col('seqcap')>=F.col('mincap'))\n",
    "df3_window = Window.partitionBy(\"InventoryKey\")\n",
    "df3 = df3.sort(F.col('squadsize').desc())\n",
    "df3_indexed = df3.withColumn(\"count\", F.count(\"*\").over(df3_window)).withColumn(\"id\", F.row_number().over(df3_window.orderBy(df3.squadsize.desc())))\n",
    "flights_window = Window.partitionBy(\"InventoryKey\")\n",
    "flights = flights.sort(F.col('seqcap').desc())\n",
    "flights_indexed = flights.withColumn(\"count\", F.count(\"*\").over(flights_window)).withColumn(\"id\", F.row_number().over(flights_window.orderBy(flights.Priority.desc())))\n",
    "assigned = flights_indexed.join(df3_indexed, ['id','InventoryKey'])\n",
    "assigned = assigned.drop('count')\n",
    "assigned = assigned.filter(F.col('seqcap')>=F.col('squadsize'))\n",
    "assigned.write.mode(\"overwrite\").saveAsTable(\"seat_assign_through\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c739cdb9-2625-4fe5-8fcc-06bb4411b425",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Expand Through Data to Leg Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8df48b0a-fa6f-4b04-98c7-1a07d061aa68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create final lockin for through flights\n",
    "testassign = spark.sql('select * from seat_assign_through')\n",
    "#calculate remainder number of seats\n",
    "testassign = testassign.withColumn('remainder', F.col('seqcap')-F.col('squadsize')) \n",
    "#if seat remainder is > 0 and squad size is >=2 lock in assignment\n",
    "lockin = testassign.filter((F.col('remainder')>=0) & (F.col('squadsize')>=2)) \n",
    "lockin = lockin.withColumn(\"Seats\", F.when(F.col('squadsize')==2,F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1]]))\n",
    "                           .when(F.col('squadsize')==3,F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1],F.col(\"seqseats\")[2]]))\n",
    "                           .when(F.col('squadsize')==4,F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1],F.col(\"seqseats\")[2],F.col(\"seqseats\")[3]]))\n",
    "                           .when(F.col('squadsize')==5,F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1],F.col(\"seqseats\")[2],F.col(\"seqseats\")[3],F.col(\"seqseats\")[4]]))\n",
    "                           .otherwise(F.col('seqseats')))\n",
    "prev_lockin = spark.sql('select * from seat_lockin_through')\n",
    "running_lockin = prev_lockin.union(lockin)\n",
    "running_lockin.write.mode(\"overwrite\").saveAsTable(\"seat_lockin_through\")\n",
    "\n",
    "'''utilize the filter for through flights. This has both InventoryKey and InventoryLegID's assigned to them. \n",
    "By exploding out the list, you can do a simple merge to take the through flight lockin to the same format \n",
    "as the regular assignment lockin. This will then put it into the proper format for final assignment \n",
    "and allow us to subtract out these seats from the individual inventory legs in the next section.'''\n",
    "\n",
    "through_merge = through.select(through.InventoryKey,F.explode(through.legs).alias('InventoryLegID'))\n",
    "lockedin = spark.sql('select * from seat_lockin_through')\n",
    "expanded = lockedin.join(through_merge, ['InventoryKey'])\n",
    "cols = spark.sql('select * from seat_lockin')\n",
    "cols = cols.columns\n",
    "expanded = expanded[[cols]]\n",
    "expanded.write.mode(\"overwrite\").option('overwriteschema','true').saveAsTable(\"seat_lockin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dda5220-1c26-4fcf-af9f-9b2dfa7750ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Single Leg Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd151a43-a9e8-44b9-b42a-3f2502d4d318",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[19]: 'This section is a repeat of the through flight family code. But is for passengers only travelling a single leg. Thus it repeats the code but substitues inventorylegid for inventorykey.'"
     ]
    }
   ],
   "source": [
    "'''This section is a repeat of the through flight family code. But is for passengers only travelling a single leg. \n",
    "Thus it repeats the code but substitues inventorylegid for inventorykey.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b07ec4f-8880-4ed3-acf0-a7bd3c289d2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Single Leg Flight Seat Group Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea17ddd5-108b-4f5e-bd2c-0beeef6e5f66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#quick clean and aggregation of the passenger info to calculate load factor\n",
    "pax = spark.sql('select * from empty_seats')\n",
    "\n",
    "lf = pax.dropDuplicates(subset = ['PassengerID','InventoryLegID']) #remove duplications\n",
    "lf = lf.groupby('InventoryLegID').count() #group passengers by flight\n",
    "lf = lf.withColumnRenamed('count','Pax') #rename of count to Pax\n",
    "\n",
    "#isolation of flight info dataset\n",
    "flight_info = pax[['DepartureDate','InventoryLegID','Capacity']] #pull inventory leg id and capacity\n",
    "flight_info = flight_info.drop_duplicates(subset = ['InventoryLegID']) #remove duplicates to get unique flights\n",
    "flight_info = flight_info.withColumn('Date', F.lit(date.today())).withColumn('DTD',F.datediff(F.col(\"DepartureDate\"),F.col('Date')))\n",
    "flight_info = flight_info.withColumnRenamed('Capacity','Equipment')\n",
    "flight_info = flight_info.drop('Date','DepartureDate')\n",
    "flight_info = flight_info.join(lf, ['InventoryLegID']) #join with count of passengers\n",
    "flight_info = flight_info.withColumn('Pax%',F.round(F.col('Pax')/F.col('Equipment'),2)) #raw % creation\n",
    "flight_info = flight_info.withColumn('LF', F.when((F.col('Pax%')<.35) & (F.col('DTD')>10), 'Low').otherwise(F.lit('High')))\n",
    "flight_info = flight_info.withColumn('LF', F.when((F.col('Pax%')<.45) & (F.col('DTD')>5), 'Low').otherwise(F.col('LF')))\n",
    "flight_info = flight_info.withColumn('LF', F.when((F.col('Pax%')<.51) & (F.col('DTD')>2), 'Low').otherwise(F.col('LF')))\n",
    "flight_info = flight_info.join(seat_map, ['Equipment','LF']) #join seatmap to flights\n",
    "allseats = flight_info.select(flight_info.DTD, flight_info.InventoryLegID,flight_info.Equipment,flight_info.LF,F.explode(flight_info.UnitDesignator)) #create unique row for each seat\n",
    "allseats = allseats.withColumnRenamed('col','UnitDesignator')\n",
    "\n",
    "taken_seats = pax.filter(F.col('UnitDesignator') != '') #filter out passengers without seats\n",
    "taken_seats = taken_seats[['InventoryLegID','UnitDesignator']] #create dataset of flight and taken seats\n",
    "taken_seats = groupset(taken_seats,taken_seats.InventoryLegID, taken_seats.UnitDesignator, 'Taken')\n",
    "taken = taken_seats.select(taken_seats.InventoryLegID,F.explode(taken_seats.Taken)) #explode list into individual rows\n",
    "taken = taken.withColumnRenamed('col','UnitDesignator')\n",
    "\n",
    "'''Take all possible seats that can be assigned and subtract out taken seats. There will be a second layer of this to subtract out seats taken by the through seat assigments.'''\n",
    "\n",
    "#remove taken seats from seat map on each flight\n",
    "availableseats = allseats.join(taken, ['InventoryLegID','UnitDesignator'], \"left_anti\")\n",
    "availableseats = seqseats(availableseats,seat_priority,'single_leg')\n",
    "\n",
    "availableseats.write.mode(\"overwrite\").saveAsTable(\"seat_flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86c82971-4c61-4200-aabe-1f8dccc65881",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' This section uses the established code for removing used seats from the final seats dataset. \n",
    "In between the assignments. It takes in the seats that are already locked in from the through \n",
    "flight assignment and subtracts them for the individual inventory legs data. \n",
    "This is the last bit of data prep to prepare for the available seats side of the main assignment.'''\n",
    "\n",
    "lockin = spark.sql('select * from seat_lockin') # pull in locked in seating assignments for through flights\n",
    "used_seats = lockin[['InventoryLegID','Row','Seats']] # pull seats that have been used\n",
    "used_seats = used_seats.withColumnRenamed('Seats','UsedSeats')\n",
    "allseats = spark.sql('select * from seat_flights') # pull in full seat list for flights.\n",
    "newseatlist = allseats.join(used_seats,['InventoryLegID','Row']) # join all and taken seats together\n",
    "newseatlist = newseatlist.withColumn('unused', F.array_except('seqseats', 'UsedSeats')) #subtract used seats\n",
    " #explode list into individual rows\n",
    "newseatlist = newseatlist.select(newseatlist.InventoryLegID,newseatlist.Row,newseatlist.Priority,newseatlist.rowcap,F.explode(newseatlist.unused))\n",
    "newseatlist = newseatlist.withColumn('Seats',F.col('col').substr(-1,1)) #pull seat letter\n",
    "newseatlist = newseatlist.groupby('InventoryLegID','Row','Priority','rowcap').agg(F.concat_ws(\",\", F.collect_list(newseatlist.col)),F.concat_ws(\",\", F.collect_list(newseatlist.Seats))) #put sum seats into row\n",
    "newseatlist = newseatlist.withColumnRenamed('concat_ws(,, collect_list(col))','EmptySeats').withColumnRenamed('concat_ws(,, collect_list(Seats))','Seats')\n",
    "newseatlist = newseatlist.withColumn('seqseats', F.col('EmptySeats')) #create new empty seats column\n",
    "newseatlist = newseatlist.withColumn('EmptySeats', F.split('EmptySeats', ',')).withColumn('seqseats', F.split('seqseats', ',')) #string to list\n",
    "newseatlist = newseatlist.withColumn('seqcap',F.size(F.col('seqseats'))) #create row capacity size\n",
    "newseatlist = newseatlist[['InventoryLegID','Row','EmptySeats','Seats','Priority','rowcap','seqcap','seqseats']]\n",
    "filt = newseatlist[['InventoryLegID','Row']]\n",
    "old_list = spark.sql('select * from seat_flights') #pull seat list\n",
    " #take original dataset and remove all rows that have been adjusted\n",
    "old_list = old_list.join(filt,['InventoryLegID','Row'], how = 'left_anti')\n",
    "availableseats = newseatlist.union(old_list) # add back in adjusted rows\n",
    "# remove any empty rows that might be in there from the roster.\n",
    "availableseats = availableseats.filter(F.col('seqcap')>0)\n",
    "availableseats.write.mode(\"overwrite\").saveAsTable(\"seat_flights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c91a614-bf9a-48f7-a4c3-f193e80a7c5d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Single Leg Passenger Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f254df63-37cd-4149-8209-f48e45afe0b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create Family Seating Groups\n",
    "familyPNR = spark.sql('select * from all_passengers')\n",
    "through_family = familyPNR.join(through_filt, ['InventoryKey'])\n",
    "through_family = through_family.groupby('PassengerID','InventoryKey').count()\n",
    "through_family = through_family.filter(F.col('count')==2).drop('count')\n",
    "# remove all the through passengers so that they are not double assigned.\n",
    "familyPNR = familyPNR.join(through_family, ['PassengerID','InventoryKey'], how = 'left_anti') \n",
    "\n",
    "df = paxgroup(familyPNR,'single_leg') # run defined function to build seat groups\n",
    "df3.write.mode(\"overwrite\").saveAsTable(\"seatgroups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c33684d-7443-4fab-8afb-a3ed679712f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Seat Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e63860a-b054-489e-94aa-acc3de42a241",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Large Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f1223ea-c5a0-4be1-8382-2d113da3f573",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3 = spark.sql('select * from seatgroups') #pull seat groups\n",
    "df3 = df3.filter(F.col('squadsize')>3)\n",
    "paxlist = df3.groupby('InventoryLegID').agg(F.min('squadsize').alias('mincap'))\n",
    "flights = spark.sql('select * from seat_flights') #pull seat list\n",
    "flights = flights.join(paxlist, ['InventoryLegID'])\n",
    "flights = flights.filter(F.col('seqcap')>=F.col('mincap'))\n",
    "df3_window = Window.partitionBy(\"InventoryLegID\")\n",
    "df3 = df3.sort(F.col('squadsize').desc())\n",
    "df3_indexed = df3.withColumn(\"count\", F.count(\"*\").over(df3_window)).withColumn(\"id\", F.row_number().over(df3_window.orderBy(df3.squadsize.desc())))\n",
    "flights_window = Window.partitionBy(\"InventoryLegID\")\n",
    "flights = flights.sort(F.col('seqcap').desc())\n",
    "flights_indexed = flights.withColumn(\"count\", F.count(\"*\").over(flights_window)).withColumn(\"id\", F.row_number().over(flights_window.orderBy(flights.seqcap.desc())))\n",
    "assigned = flights_indexed.join(df3_indexed, ['id','InventoryLegID'])\n",
    "assigned = assigned.drop('count')\n",
    "assigned = assigned.filter(F.col('seqcap')>=F.col('squadsize'))\n",
    "assigned.write.mode(\"overwrite\").option('overwriteschema', 'true').saveAsTable(\"seat_assign\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33541e68-3996-45b1-9e78-0be6d43c289e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' data clean section to prep available seats for next round of assignments'''\n",
    "testassign = spark.sql('select * from seat_assign')\n",
    "testassign = testassign.withColumn('remainder', F.col('seqcap')-F.col('squadsize')) #calculate remainder number of seats\n",
    "#if seat remainder is > 0 and squad size is >=2 lock in assignment\n",
    "lockin = testassign.filter((F.col('remainder')>=0) & (F.col('squadsize')>=2)) \n",
    "lockin = lockin.withColumn(\"Seats\", F.when(F.col('squadsize')==2,F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1]]))\n",
    "                           .when(F.col('squadsize')==3,F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1],F.col(\"seqseats\")[2]]))\n",
    "                           .when(F.col('squadsize')==4,F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1],F.col(\"seqseats\")[2],F.col(\"seqseats\")[3]]))\n",
    "                           .when(F.col('squadsize')==5,F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1],F.col(\"seqseats\")[2],F.col(\"seqseats\")[3],F.col(\"seqseats\")[4]]))\n",
    "                           .otherwise(F.col('seqseats')))\n",
    "prev_lockin = spark.sql('select * from seat_lockin')\n",
    "running_lockin = prev_lockin.union(lockin)\n",
    "running_lockin.write.mode(\"overwrite\").saveAsTable(\"seat_lockin\")\n",
    "lockin = spark.sql('select * from seat_lockin')\n",
    "used_seats = lockin[['InventoryLegID','Row','Seats']]\n",
    "used_seats = used_seats.withColumnRenamed('Seats','UsedSeats')\n",
    "allseats = spark.sql('select * from seat_flights')\n",
    "newseatlist = allseats.join(used_seats,['InventoryLegID','Row'])\n",
    "newseatlist = newseatlist.withColumn('unused', F.array_except('seqseats', 'UsedSeats'))\n",
    "newseatlist = newseatlist.select(newseatlist.InventoryLegID,newseatlist.Row,newseatlist.Priority,newseatlist.rowcap,F.explode(newseatlist.unused)) #explode list into individual rows\n",
    "newseatlist = newseatlist.withColumn('Seats',F.col('col').substr(-1,1)) #pull seat letter\n",
    "newseatlist = newseatlist.groupby('InventoryLegID','Row','Priority','rowcap').agg(F.concat_ws(\",\", F.collect_list(newseatlist.col)),F.concat_ws(\",\", F.collect_list(newseatlist.Seats))) #put sum seats into row\n",
    "newseatlist = newseatlist.withColumnRenamed('concat_ws(,, collect_list(col))','EmptySeats').withColumnRenamed('concat_ws(,, collect_list(Seats))','Seats')\n",
    "newseatlist = newseatlist.withColumn('seqseats', F.col('EmptySeats'))\n",
    "newseatlist = newseatlist.withColumn('EmptySeats', F.split('EmptySeats', ',')).withColumn('seqseats', F.split('seqseats', ',')) #string to list\n",
    "newseatlist = newseatlist.withColumn('seqcap',F.size(F.col('seqseats'))) #create row capacity size\n",
    "newseatlist = newseatlist[['InventoryLegID','Row','EmptySeats','Seats','Priority','rowcap','seqcap','seqseats']]\n",
    "filt = newseatlist[['InventoryLegID','Row']]\n",
    "old_list = spark.sql('select * from seat_flights') #pull seat list\n",
    "old_list = old_list.join(filt,['InventoryLegID','Row'], how = 'left_anti')\n",
    "availableseats = newseatlist.union(old_list)\n",
    "availableseats = availableseats.filter(F.col('seqcap')>0)\n",
    "availableseats.write.mode(\"overwrite\").saveAsTable(\"seat_flights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a3414b7-88d2-4739-94c5-cf45dc73bb81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Main Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff5497cd-f089-4742-a51e-9f0e26371edf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3 = spark.sql('select * from seatgroups') #pull seat groups\n",
    "df3 = df3.filter(F.col('squadsize')<4)\n",
    "paxlist = df3.groupby('InventoryLegID').agg(F.min('squadsize').alias('mincap'))\n",
    "flights = spark.sql('select * from seat_flights') #pull seat list\n",
    "flights = flights.withColumn(\"seqseats\", \\\n",
    "    F.when(F.col('Seats').contains('A,B,C'),F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1],F.col(\"seqseats\")[2]]))\n",
    "    .when(F.col('Seats').contains('F,E,D'),F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1],F.col(\"seqseats\")[2]]))\n",
    "    .when(F.col('Seats').contains('C,B,A'),F.array([F.element_at(F.col('seqseats'), -3), F.element_at(F.col('seqseats'), -2),F.element_at(F.col('seqseats'), -1)]))\n",
    "    .when(F.col('Seats').contains('D,E,F'),F.array([F.element_at(F.col('seqseats'), -3), F.element_at(F.col('seqseats'), -2),F.element_at(F.col('seqseats'), -1)]))\n",
    "    .when(F.col('Seats').contains('B,C'),F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1]]))\n",
    "    .when(F.col('Seats').contains('E,D'),F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1]]))\n",
    "    .when(F.col('Seats').contains('C,B'),F.array([ F.element_at(F.col('seqseats'), -2),F.element_at(F.col('seqseats'), -1)]))\n",
    "    .when(F.col('Seats').contains('D,E'),F.array([ F.element_at(F.col('seqseats'), -2),F.element_at(F.col('seqseats'), -1)]))\n",
    "    .when(F.col('Seats').contains('D,C'),F.array([F.col(\"seqseats\")[0]]))\n",
    "    .when(F.col('Seats').contains('C,D'),F.array([F.col(\"seqseats\")[0]]))\n",
    "    .otherwise(F.col(\"seqseats\")))\n",
    "flights = flights.withColumn('seqcap',F.size(F.col('seqseats'))) #create row capacity size\n",
    "flights = flights.join(paxlist, ['InventoryLegID'])\n",
    "flights = flights.filter(F.col('seqcap')>=F.col('mincap'))\n",
    "df3_window = Window.partitionBy(\"InventoryLegID\")\n",
    "df3 = df3.sort(F.col('squadsize').desc())\n",
    "df3_indexed = df3.withColumn(\"count\", F.count(\"*\").over(df3_window)).withColumn(\"id\", F.row_number().over(df3_window.orderBy(df3.squadsize.desc())))\n",
    "flights_window = Window.partitionBy(\"InventoryLegID\")\n",
    "flights = flights.sort(F.col('seqcap').desc())\n",
    "flights_indexed = flights.withColumn(\"count\", F.count(\"*\").over(flights_window)).withColumn(\"id\", F.row_number().over(flights_window.orderBy(flights.Priority.desc())))\n",
    "assigned = flights_indexed.join(df3_indexed, ['id','InventoryLegID'])\n",
    "assigned = assigned.drop('count')\n",
    "assigned = assigned.filter(F.col('seqcap')>=F.col('squadsize'))\n",
    "assigned.write.mode(\"overwrite\").saveAsTable(\"seat_assign\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b269db74-62d5-4247-8ba6-271e0454f54b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "testassign = spark.sql('select * from seat_assign')\n",
    "testassign = testassign.withColumn('remainder', F.col('seqcap')-F.col('squadsize')) #calculate remainder number of seats\n",
    "lockin = testassign.filter((F.col('remainder')>=0) & (F.col('squadsize')>=2)) #if seat remainder is > 0 and squad size is >=2 lock in assignment\n",
    "lockin = lockin.withColumn(\"Seats\", F.when(F.col('squadsize')==2,F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1]])).when(F.col('squadsize')==3,F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1],F.col(\"seqseats\")[2]])).when(F.col('squadsize')==4,F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1],F.col(\"seqseats\")[2],F.col(\"seqseats\")[3]])).when(F.col('squadsize')==5,F.array([F.col(\"seqseats\")[0], F.col(\"seqseats\")[1],F.col(\"seqseats\")[2],F.col(\"seqseats\")[3],F.col(\"seqseats\")[4]])).otherwise(F.col('seqseats')))\n",
    "prev_lockin = spark.sql('select * from seat_lockin')\n",
    "running_lockin = prev_lockin.union(lockin)\n",
    "running_lockin = running_lockin.dropDuplicates()\n",
    "running_lockin.write.mode(\"overwrite\").saveAsTable(\"seat_lockin\")\n",
    "lockin = spark.sql('select * from seat_lockin')\n",
    "used_seats = lockin[['InventoryLegID','Row','Seats']]\n",
    "used_seats = used_seats.withColumnRenamed('Seats','UsedSeats')\n",
    "allseats = spark.sql('select * from seat_flights')\n",
    "newseatlist = allseats.join(used_seats,['InventoryLegID','Row'])\n",
    "newseatlist = newseatlist.withColumn('unused', F.array_except('seqseats', 'UsedSeats'))\n",
    "newseatlist = newseatlist.select(newseatlist.InventoryLegID,newseatlist.Row,newseatlist.Priority,newseatlist.rowcap,F.explode(newseatlist.unused)) #explode list into individual rows\n",
    "newseatlist = newseatlist.withColumn('Seats',F.col('col').substr(-1,1)) #pull seat letter\n",
    "newseatlist = newseatlist.groupby('InventoryLegID','Row','Priority','rowcap').agg(F.concat_ws(\",\", F.collect_list(newseatlist.col)),F.concat_ws(\",\", F.collect_list(newseatlist.Seats))) #put sum seats into row\n",
    "newseatlist = newseatlist.withColumnRenamed('concat_ws(,, collect_list(col))','EmptySeats').withColumnRenamed('concat_ws(,, collect_list(Seats))','Seats')\n",
    "newseatlist = newseatlist.withColumn('seqseats', F.col('EmptySeats'))\n",
    "newseatlist = newseatlist.withColumn('EmptySeats', F.split('EmptySeats', ',')).withColumn('seqseats', F.split('seqseats', ',')) #string to list\n",
    "newseatlist = newseatlist.withColumn('seqcap',F.size(F.col('seqseats'))) #create row capacity size\n",
    "newseatlist = newseatlist[['InventoryLegID','Row','EmptySeats','Seats','Priority','rowcap','seqcap','seqseats']]\n",
    "filt = newseatlist[['InventoryLegID','Row']]\n",
    "old_list = spark.sql('select * from seat_flights') #pull seat list\n",
    "old_list = old_list.join(filt,['InventoryLegID','Row'], how = 'left_anti')\n",
    "availableseats = newseatlist.union(old_list)\n",
    "availableseats = availableseats.filter(F.col('seqcap')>0)\n",
    "availableseats.write.mode(\"overwrite\").saveAsTable(\"seat_flights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd13f423-e268-4286-a7c9-956e450e6f29",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Pax Number Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e7a7b2d-4244-4b9c-bc9a-788d7d45dde7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filt = spark.sql('select * from seat_lockin')\n",
    "filt = filt[['RecordLocator']]\n",
    "filt = filt.dropDuplicates(subset = ['RecordLocator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7800458c-e300-48b5-91ed-62a58ed82647",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#paxnumber query\n",
    "familyPNR_query = \"\"\" sql query for what record locators will be assigned\"\"\"\n",
    "familyPNR = spark.read.jdbc(url=jdbcUrl, table=familyPNR_query)\n",
    "familyPNR.write.mode(\"overwrite\").saveAsTable(\"pax_number_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36e882d3-1cc8-47b7-80a0-667033571160",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pax_numbers = spark.sql('select PassengerID,RecordLocator from pax_number_raw')\n",
    "pax_numbers = pax_numbers.join(filt, ['RecordLocator'])\n",
    "pax_numbers.write.mode(\"overwrite\").saveAsTable(\"pax_number_raw\")\n",
    "pax_numbers = spark.sql('select PassengerID,RecordLocator from pax_number_raw')\n",
    "pax_numbers = paxnumbers(pax_numbers)\n",
    "pax_numbers.write.mode(\"overwrite\").saveAsTable(\"pax_numbers\") #assign numbers to holdout dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c71cca3-cccf-4f38-b708-ebbbdcc180ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Final Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e607ec16-ea2a-4d16-b4ba-baf9193b8bcb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "running_lockin = spark.sql('select * from seat_lockin')\n",
    "seats = running_lockin.select(running_lockin.InventoryLegID,running_lockin.RecordLocator,F.explode(running_lockin.Seats)) #explode seats\n",
    "peoples = running_lockin.select(running_lockin.InventoryLegID,running_lockin.RecordLocator,F.explode(running_lockin.the_squad)) #explode passengers\n",
    "seats = seats.withColumn(\"id\", F.monotonically_increasing_id()) #unique id\n",
    "seats = seats.withColumnRenamed(\"col\", 'UnitDesignator')\n",
    "peoples = peoples.withColumn(\"id\", F.monotonically_increasing_id()) #unique id\n",
    "peoples = peoples.withColumnRenamed(\"col\", 'PassengerID')\n",
    "final = seats.join(peoples,['id','InventoryLegID','RecordLocator']) #join seats to people\n",
    "final = final.dropDuplicates() #drop duplications\n",
    "numbers = spark.sql('select * from pax_numbers') #pull in passenger number\n",
    "numbers = numbers.withColumn('PassengerNumber',F.col('PassengerNumber').astype(StringType()))\n",
    "final = final.join(numbers,['RecordLocator','PassengerID']) #merge dataset to add passenger number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0a40a0e-be82-494a-852d-5c428d0a2f61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "familyPNR_query = \"\"\" insert query for flight identification information\"\"\"\n",
    "\n",
    "familyPNR = spark.read.jdbc(url=jdbcUrl, table=familyPNR_query)\n",
    "familyPNR.write.mode(\"overwrite\").option('overwriteschema','true').saveAsTable(\"pax_info_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55e53f83-8a54-4660-9b76-daa32cce7c45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "info_raw = spark.sql('select * from pax_info_raw')\n",
    "flightdata = info_raw[['InventoryLegID','DepartureDate','FlightNumber','DepartureStation','ArrivalStation','STD','Capacity']]\n",
    "flightdata = flightdata.dropDuplicates(subset = ['InventoryLegID'])\n",
    "\n",
    "seat_loc = spark.sql('select * from seat_loc')\n",
    "seat_loc = seat_loc.withColumnRenamed('LocationConcat','SeatLoc').withColumnRenamed('Seat','UnitDesignator')\n",
    "final = final.join(flightdata, ['InventoryLegID'])  #join flight data\n",
    "final = final.withColumnRenamed('RecordLocator','PNR')\n",
    "final = final.withColumn('STDdate',F.to_date(F.col('STD'))).withColumn('STDtime', F.date_format('STD', 'HH:mm:ss'))\n",
    "final = final.withColumn('STDdate',F.col('STDdate').cast(StringType())).withColumn('STDtime',F.col('STDtime').cast(StringType()))\n",
    "final = final.withColumn('STD', F.concat(F.col('STDdate'),F.lit('T'), F.col('STDtime')))\n",
    "final = final.withColumn('FlightNumber', F.col('FlightNumber').cast(StringType()))\n",
    "final = final[['PNR','PassengerID','DepartureDate','DepartureStation','STD','ArrivalStation','FlightNumber','InventoryLegID','UnitDesignator','PassengerNumber']]\n",
    "final = final.dropDuplicates()\n",
    "final.write.mode(\"overwrite\").saveAsTable(\"final_seat_assign\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Orion v4 (Seat Assignment)",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
